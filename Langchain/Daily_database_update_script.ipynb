{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c64d689f-9560-46ee-9952-b1158262174b",
   "metadata": {},
   "source": [
    "# 1. Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88ffc522-05f2-4c86-bbd6-aa88d567fb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from langchain_core.documents import Document\n",
    "from typing import List\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8318c0-9bd7-4301-a229-2e46bed22486",
   "metadata": {},
   "source": [
    "# 2. Scrape the latest articles (12 articles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91c2ae3a-64fa-49e9-b595-affae01c15a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始获取文章列表...\n",
      "正在请求第 1 次 API...\n",
      "  - 获取到 6 篇文章\n",
      "  - 更新 offset 为: 1770032606\n",
      "正在请求第 2 次 API...\n",
      "  - 获取到 6 篇文章\n",
      "  - 更新 offset 为: 1769847711\n",
      "  - 达到最大请求数 2，停止请求。\n",
      "\n",
      "--- 总共获取到 12 篇文章列表 ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://web-data.api.hk01.com/v2/issues/10221/relatedBlock/0/\"\n",
    "\n",
    "headers = {\n",
    "    'Referer': 'https://www.hk01.com/',\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'limit': 6, #每次六篇\n",
    "    'offset': 0,  # 初始 offset\n",
    "    'bucketId': '00000'\n",
    "}\n",
    "\n",
    "# save all the articles\n",
    "all_articles = []\n",
    "\n",
    "# 循环两次爬取12篇最新的文章\n",
    "request_count = 0\n",
    "max_requests = 2\n",
    "\n",
    "print(\"开始获取文章列表...\")\n",
    "while True:\n",
    "    print(f\"正在请求第 {request_count + 1} 次 API...\")\n",
    "    \n",
    "    # 发送 GET 请求\n",
    "    response = requests.get(base_url, headers=headers, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        \n",
    "        # 提取 items\n",
    "        items = data.get('items', [])\n",
    "        next_offset = data.get('nextOffset', None)\n",
    "        \n",
    "        print(f\"  - 获取到 {len(items)} 篇文章\")\n",
    "        \n",
    "        # 将本次获取的文章添加到总列表\n",
    "        all_articles.extend(items)\n",
    "        \n",
    "        # 检查是否有 nextOffset\n",
    "        if next_offset is not None:\n",
    "            # 更新 params 中的 offset 为 nextOffset\n",
    "            params['offset'] = next_offset\n",
    "            print(f\"  - 更新 offset 为: {next_offset}\")\n",
    "        else:\n",
    "            # 如果没有 nextOffset，说明数据已加载完毕\n",
    "            print(\"  - 没有更多数据，停止请求。\")\n",
    "            break\n",
    "        \n",
    "        # 检查是否达到最大请求数\n",
    "        request_count += 1\n",
    "        if request_count >= max_requests:\n",
    "            print(f\"  - 达到最大请求数 {max_requests}，停止请求。\")\n",
    "            break\n",
    "        \n",
    "        # 添加短暂延迟，避免请求过于频繁\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "    else:\n",
    "        print(f\"  - 请求失败，状态码: {response.status_code}\")\n",
    "        print(response.text) # 打印错误信息\n",
    "        break\n",
    "\n",
    "print(f\"\\n--- 总共获取到 {len(all_articles)} 篇文章列表 ---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e511177-f6ea-4402-9fe6-69102fc1dcd9",
   "metadata": {},
   "source": [
    "# 3. This script will run by default every day at 0:00, therefore it will filter out news from yesterday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c9fab5e-1cb4-4071-bc13-5ca56abe22e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检查第 1 篇文章: 尖沙咀見這東西不要撿！港男遇「觸碰型陷阱」騙財　機智1招脫困..., 发布日期: 2026-02-05\n",
      "  -> 不是昨天的新闻，跳过。\n",
      "检查第 2 篇文章: 緬北四大家族覆滅　明家白家伏法　中國用三年徹底剷除電騙毒瘤？..., 发布日期: 2026-02-04\n",
      "  -> 符合条件，加入处理列表。\n",
      "检查第 3 篇文章: 10萬元器械植入體內離奇消失？河南醫生詐騙94名患者判囚12..., 发布日期: 2026-02-04\n",
      "  -> 符合条件，加入处理列表。\n",
      "检查第 4 篇文章: 新型碰瓷黨｜醫生及索償人共4人涉騙被捕　警檢名錶金器拖走的士..., 发布日期: 2026-02-04\n",
      "  -> 符合条件，加入处理列表。\n",
      "检查第 5 篇文章: 日本首例！歌舞伎町王牌牛郎交友App非法拉客被捕　扮IT男騙..., 发布日期: 2026-02-04\n",
      "  -> 符合条件，加入处理列表。\n",
      "检查第 6 篇文章: 網騙2026：當攻擊進入Agentic AI時代，CIO 應..., 发布日期: 2026-02-04\n",
      "  -> 符合条件，加入处理列表。\n",
      "检查第 7 篇文章: 一周110宗經Carousell網購騙案呃過千萬　有人賣$1..., 发布日期: 2026-02-02\n",
      "  -> 不是昨天的新闻，跳过。\n",
      "检查第 8 篇文章: 網上情緣騙案｜扮女人氹落疊稱贈遺產　冒警二次詐騙　共呃440..., 发布日期: 2026-02-02\n",
      "  -> 不是昨天的新闻，跳过。\n",
      "检查第 9 篇文章: 警反詐騙拘682人涉$6.2億　詐騙集團聘洗黑錢集團洗白$4..., 发布日期: 2026-02-02\n",
      "  -> 不是昨天的新闻，跳过。\n",
      "检查第 10 篇文章: 新型詐騙？2男女稱大學玩遊戲求傳自拍照　網民點出破綻：係騙案..., 发布日期: 2026-02-02\n",
      "  -> 不是昨天的新闻，跳过。\n",
      "检查第 11 篇文章: 緬北電騙白家案　白所成一審後病亡　白應蒼等4人伏法..., 发布日期: 2026-02-02\n",
      "  -> 不是昨天的新闻，跳过。\n",
      "检查第 12 篇文章: 內地23歲男廚師冒充殲-16機師　與上百名女子戀愛圖呃錢被捕..., 发布日期: 2026-02-01\n",
      "  -> 不是昨天的新闻，跳过。\n"
     ]
    }
   ],
   "source": [
    "# 计算昨天的日期（假设你的系统时间是香港时间 UTC+8）\n",
    "yesterday = (datetime.now() - timedelta(days=1)).date()\n",
    "# 转为 \"YYYY-MM-DD\" 字符串\n",
    "yesterday_str = (datetime.now() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# 用于存储昨天的文章\n",
    "yesterday_articles = []\n",
    "\n",
    "output_filename = \"daily_new_articles.md\"\n",
    "# 遍历所有获取到的文章\n",
    "for i, item in enumerate(all_articles, 1):\n",
    "    article_data = item.get('data', {})\n",
    "    title = article_data.get('title', '无标题')\n",
    "    url = article_data.get('canonicalUrl', '无链接')\n",
    "    publish_timestamp = article_data.get('publishTime', None)\n",
    "\n",
    "    # 转换时间戳为香港时间（如果有）\n",
    "    publish_time_hk = \"未知\"\n",
    "    pub_date_hk = None\n",
    "    if publish_timestamp:\n",
    "        try:\n",
    "            dt_utc = datetime.utcfromtimestamp(publish_timestamp)\n",
    "            dt_hk = dt_utc + timedelta(hours=8)  # 转为香港时间\n",
    "            publish_time_hk = dt_hk.strftime(\"%Y-%m-%d %H:%M:%S\")  # 格式化完整时间\n",
    "            pub_date_hk = dt_hk.date()  # 提取日期部分\n",
    "        except Exception as e:\n",
    "            print(f\"  - 时间戳转换失败: {publish_timestamp}, 错误: {e}\")\n",
    "            # 如果转换失败，无法判断日期，跳过此篇文章\n",
    "            continue\n",
    "\n",
    "    print(f\"检查第 {i} 篇文章: {title[:30]}..., 发布日期: {pub_date_hk}\")\n",
    "\n",
    "    # 只处理昨天发布的文章\n",
    "    if pub_date_hk == yesterday:\n",
    "        print(f\"  -> 符合条件，加入处理列表。\")\n",
    "        yesterday_articles.append({\n",
    "            'index': len(yesterday_articles) + 1, # 重新编号\n",
    "            'item': item,\n",
    "            'title': title,\n",
    "            'url': url,\n",
    "            'publish_time_hk': publish_time_hk\n",
    "        })\n",
    "    else:\n",
    "        print(f\"  -> 不是昨天的新闻，跳过。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f5be4c-5479-4f53-8186-be4c5d000c41",
   "metadata": {},
   "source": [
    "# 4. Save all yesterday's articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baf279a8-4cec-47f9-a806-59b41ece18c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 找到 5 篇昨天的文章，开始写入文件... ---\n",
      "正在处理昨天的第 1 篇文章: 緬北四大家族覆滅　明家白家伏法　中國用三年徹底剷除電騙毒瘤？...\n",
      "正在处理昨天的第 2 篇文章: 10萬元器械植入體內離奇消失？河南醫生詐騙94名患者判囚12...\n",
      "正在处理昨天的第 3 篇文章: 新型碰瓷黨｜醫生及索償人共4人涉騙被捕　警檢名錶金器拖走的士...\n",
      "正在处理昨天的第 4 篇文章: 日本首例！歌舞伎町王牌牛郎交友App非法拉客被捕　扮IT男騙...\n",
      "正在处理昨天的第 5 篇文章: 網騙2026：當攻擊進入Agentic AI時代，CIO 應...\n",
      "\n",
      "--- Markdown 文件 'daily_new_articles.md' 写入完成！仅包含 2026-02-04 的 5 篇文章。---\n"
     ]
    }
   ],
   "source": [
    "# 如果没有找到昨天的文章，则不创建文件\n",
    "if not yesterday_articles:\n",
    "    print(f\"\\n--- 没有找到 {yesterday} 的文章，跳过写入文件。---\")\n",
    "else:\n",
    "    print(f\"\\n--- 找到 {len(yesterday_articles)} 篇昨天的文章，开始写入文件... ---\")\n",
    "    # 现在开始写入 Markdown 文件\n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"共 {len(yesterday_articles)} 篇（仅 {yesterday} 发布）\\n\\n\")\n",
    "\n",
    "        for article_obj in yesterday_articles:\n",
    "            i = article_obj['index']\n",
    "            item = article_obj['item']\n",
    "            title = article_obj['title']\n",
    "            url = article_obj['url']\n",
    "            publish_time_hk = article_obj['publish_time_hk']\n",
    "\n",
    "            print(f\"正在处理昨天的第 {i} 篇文章: {title[:30]}...\")\n",
    "\n",
    "            try:\n",
    "                # 发送 GET 请求获取文章页面内容\n",
    "                response = requests.get(url)\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "                    # 提取正文 (找到所有 class 包含 \"whitespace-pre-wrap break-words\" 的 <p> 标签)\n",
    "                    paragraphs = soup.find_all('p', class_='whitespace-pre-wrap break-words')\n",
    "                    content = \"\\n\\n\".join([p.get_text(strip=True) for p in paragraphs])\n",
    "\n",
    "                    # 写入 Markdown 格式（添加发布时间）\n",
    "                    f.write(f\"## [{i}. {title}]({url})\\n\\n\")\n",
    "                    f.write(f\"**发布时间（香港时间）**: {publish_time_hk}\\n\\n\")  # 添加时间信息\n",
    "                    f.write(f\"{content}\\n\\n---\\n\\n\") # 使用 --- 作为文章分隔符\n",
    "\n",
    "                else:\n",
    "                    print(f\"  - 请求文章失败，状态码: {response.status_code}\")\n",
    "                    # 即使请求失败，也写入标题、链接和时间，标记错误\n",
    "                    f.write(f\"## [{i}. {title}]({url})\\n\\n\")\n",
    "                    f.write(f\"**发布时间（香港时间）**: {publish_time_hk}\\n\\n\")  # 添加时间信息\n",
    "                    f.write(f\"[Error: Failed to fetch content, status code {response.status_code}]\\n\\n---\\n\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  - 抓取文章时出错: {e}\")\n",
    "                # 即使出错，也写入标题、链接和时间，标记错误\n",
    "                f.write(f\"## [{i}. {title}]({url})\\n\\n\")\n",
    "                f.write(f\"**发布时间（香港时间）**: {publish_time_hk}\\n\\n\")  # 添加时间信息\n",
    "                f.write(f\"[Error: {str(e)}]\\n\\n---\\n\\n\")\n",
    "\n",
    "    print(f\"\\n--- Markdown 文件 '{output_filename}' 写入完成！仅包含 {yesterday} 的 {len(yesterday_articles)} 篇文章。---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af025624-426a-4509-97c8-67a4c4ed1195",
   "metadata": {},
   "source": [
    "# 5. Convert the article into a document and store it in the existing vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43ea8f31-2e21-4a25-8b5b-6d42d81c50a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共加载 5 篇诈骗新闻\n"
     ]
    }
   ],
   "source": [
    "file_path = \"daily_new_articles.md\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "# 按 \"---\" 分割文章（支持前后有空行）\n",
    "articles = [a.strip() for a in re.split(r'\\n-{3,}\\n', content) if a.strip()]\n",
    "\n",
    "print(f\"共加载 {len(articles)} 篇诈骗新闻\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcdf6ee-6cb9-42dc-8879-43457c3d55c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "docs = []\n",
    "for i, art in enumerate(articles):\n",
    "    # 提取标题（可选优化）\n",
    "    lines = art.splitlines()\n",
    "    title = lines[0].strip().lstrip('#').strip() if lines else f\"文章 #{i+1}\"\n",
    "    doc = Document(\n",
    "        page_content=art,\n",
    "        metadata={\n",
    "            \"source\": \"daily_new_articles.md\",\n",
    "            \"article_id\": i + 1,\n",
    "            \"title\": title,\n",
    "            \"date\": yesterday_str\n",
    "        }\n",
    "    )\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcec449-77cd-4ad3-9f33-3ab63c72c7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalEmbeddings:\n",
    "    def __init__(self, model_name=\"shibing624/text2vec-base-chinese\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return self.model.encode(texts, normalize_embeddings=True).tolist()\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self.model.encode([text], normalize_embeddings=True)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f417b387-0b45-4715-93e9-bfc183e1be98",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = LocalEmbeddings()\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=\"./chroma_hk01_scam_db\",\n",
    "    embedding_function=embedding_model\n",
    ")\n",
    "vectorstore.add_documents(docs)\n",
    "\n",
    "print(\"更新完成！RAG 系统现在包含最新诈骗新闻。\")\n",
    "print(f\"写入后文档总数: {vectorstore._collection.count()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
